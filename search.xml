<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>杂记 2023年3-6月辑</title>
      <link href="/2023/07/24/gossip-5/"/>
      <url>/2023/07/24/gossip-5/</url>
      
        <content type="html"><![CDATA[<h3 id="20230304"><a href="#20230304" class="headerlink" title="20230304"></a>20230304</h3><p>挤了两天地铁，终于知道什么叫早高峰了。</p><p>明明什么都没干，但很疲惫。</p><h3 id="20230311"><a href="#20230311" class="headerlink" title="20230311"></a>20230311</h3><p>果然当学生是最快乐的。</p><h3 id="20230312"><a href="#20230312" class="headerlink" title="20230312"></a>20230312</h3><p>又给网站换了个主题。相比之前操作起来更繁琐了。</p><h3 id="20230313"><a href="#20230313" class="headerlink" title="20230313"></a>20230313</h3><p>周一了……双休根本不够休啊。</p><p>我早上出门的时候想干啥来着？话说想不起来自己要做什么事情的时候，能不能不要想起自己要做什么事情这件事啊。</p><h3 id="20230314"><a href="#20230314" class="headerlink" title="20230314"></a>20230314</h3><p>早上又想起来前一天早上想干什么了。真是神奇。其实想到做什么事情的话，最好还是尽快去做，一下忘了就指不定什么时候能再想起来了。</p><h3 id="20230315"><a href="#20230315" class="headerlink" title="20230315"></a>20230315</h3><p>结束了疲惫的一天，准备去食堂吃点好的。到食堂发现太晚了，汉堡窗口只有鳕鱼堡了。但是还有两个，全买了。</p><p>我只能说鳕鱼堡剩到最后不是没有道理的。</p><p>我为啥要买两个？</p><p>至少吃饱了。但是鳕鱼堡比其它种类贵啊，我不甘心😭。</p><h3 id="20230317"><a href="#20230317" class="headerlink" title="20230317"></a>20230317</h3><p>真寻好可爱啊。真可爱啊。太可爱啦！</p><h3 id="20230329"><a href="#20230329" class="headerlink" title="20230329"></a>20230329</h3><p>速成初稿。希望中检顺利吧。</p><h3 id="20230330"><a href="#20230330" class="headerlink" title="20230330"></a>20230330</h3><p>想学的东西太多，但不知道怎么安排时间。</p><p>有点疲惫。其实并没有那么多的事情，但总感觉精力被一点点的消磨殆尽。</p><h3 id="20230408"><a href="#20230408" class="headerlink" title="20230408"></a>20230408</h3><p>最近又忙又累。</p><p>今天看了《玲芽之旅》，记录一下。</p><p>顺便这导航导的什么路，黑灯瞎火的也没人行道。</p><h3 id="20230414"><a href="#20230414" class="headerlink" title="20230414"></a>20230414</h3><p>人的适应力真强啊。每天晚睡第二天还能在闹钟响起前醒过来，都有生物钟了。</p><p>难道自己是不喜欢改变的那种人？也不像。人还真是矛盾啊。</p><p>虽然早上能早起，但并不意味着睡够了。我甚至能坐着睡觉。如果不是地铁上睡着容易坐过站的话，估计我能练成站着睡觉的本领吧。</p><p>好像说的不对，哪能叫坐过站呢？应该是站过站。哈哈。</p><h3 id="20230415"><a href="#20230415" class="headerlink" title="20230415"></a>20230415</h3><p>困。睡！</p><h3 id="20230421"><a href="#20230421" class="headerlink" title="20230421"></a>20230421</h3><p>事情总是挤在一起出现，很让人心烦。其实一件一件分开来做都不难，但堆在一起总能让人直接失去解决的念头，就像现在一边上班还一边得在10天内写完终稿一样。10天里有7天在上班，谁下班回来还有心情写稿子啊。</p><p>但还是不得不写。真是不知道为什么如此着急收稿子，收完之后这个5月和6月都在干嘛……</p><h3 id="20230425"><a href="#20230425" class="headerlink" title="20230425"></a>20230425</h3><p>总算把论文按上周的要求改好了。我前两天为什么开摆，整得今天这么累。想早点下班。</p><h3 id="20230426"><a href="#20230426" class="headerlink" title="20230426"></a>20230426</h3><p>下班直接无缝衔接组会。晚饭泡汤喽。</p><h3 id="20230428"><a href="#20230428" class="headerlink" title="20230428"></a>20230428</h3><p>上班，下午突然开始下雨，接着下冰雹。还是第一次亲眼见到冰雹。没带伞，淋雨回去了。</p><p>伤心的一天。😢</p><p>被天气预报骗了。下班准备出门前看预报说20分钟后变大雨，心想不如直接淋雨出去算了，结果走了一半抬头发现又变了……进地铁站发现鞋底还掉了一块，也不能又返回去找。到了站台上发现全是人，车也挤不上去。上了车发现根本没信号，消息也发不出去……</p><p>开心的是今天有位置坐，但是坐了一站就下车了。</p><h3 id="20230507"><a href="#20230507" class="headerlink" title="20230507"></a>20230507</h3><p>真是时不时就要感慨世界之大与物种多样性……怎么会专门有人跑到别人的MC服务器里把图给炸了的啊？什么玩意。这下要重置人生了。</p><p>开正版验证都拦不住小鬼吗？以后要开白名单了。</p><p>小鬼信息如下：</p><p>{“name”:”Haim1337”,”uuid”:”698afd58-653b-46d8-b418-ec93b031cc30”,”expiresOn”:”2023-05-19 21:07:27 +0800”},{“name”:”Tomijones”,”uuid”:”b5f0c523-e37e-4ce3-9560-7ae6bd386ab8”,”expiresOn”:”2023-05-19 20:56:50 +0800”},</p><h3 id="20230510"><a href="#20230510" class="headerlink" title="20230510"></a>20230510</h3><p>今天楼梯间一股榴莲味，什么情况。</p><h3 id="20230526"><a href="#20230526" class="headerlink" title="20230526"></a>20230526</h3><p>这什么ARMOURY CRATE一直更新失败，搞了个把月都没修好。一直更新-重启-更新-重启无限循环，拿我当消遣呢？把官网给的方法全试了一遍没一个成功的，包括卸载重下，甚至用的话说自己出的卸载软件。浪费了老子人生中的不知道几个小时了。</p><p><strong>现在风扇就处于一个乱转的状态，它想转就转，还转得特大声。</strong></p><h3 id="20230603"><a href="#20230603" class="headerlink" title="20230603"></a>20230603</h3><p>倒转乾坤为什么不能对物品使用呢👿？如果能对呀哈哈的果实用一下多好啊，从哪来回哪去。</p><h3 id="20230629"><a href="#20230629" class="headerlink" title="20230629"></a>20230629</h3><p>坐高铁被收了把剪刀，可这剪刀明明是我以前坐高铁带来的……不理解。剪刀我的剪刀😭。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>杂记 2023年2月辑</title>
      <link href="/2023/02/28/gossip-4/"/>
      <url>/2023/02/28/gossip-4/</url>
      
        <content type="html"><![CDATA[<h3 id="20230201"><a href="#20230201" class="headerlink" title="20230201"></a>20230201</h3><p>听说今天是彗星C&#x2F;2022E3（ZTF）观测的好时机：彗星到达近地点，或许能在夜空中肉眼可见。于是出门一看，发现连月亮都见不到。</p><p>原来今天晚上是阴天。</p><h3 id="20230205"><a href="#20230205" class="headerlink" title="20230205"></a>20230205</h3><p>更新网站插件显示失败，一刷新页面直接显示500了。</p><p>人都傻了，直接关闭浏览器。</p><p>思考了好一会接下来怎么办后，突然发现网站又神奇地能打开了。万幸。</p><h3 id="20230207"><a href="#20230207" class="headerlink" title="20230207"></a>20230207</h3><p>虹夏，太可爱啦虹夏！</p><p>🥰人活着哪有不喜欢虹夏的，硬撑罢了！🥰人活着哪有不喜欢虹夏的，硬撑罢了！🥰人活着哪有不喜欢虹夏的，硬撑罢了！🥰人活着哪有不喜欢虹夏的，硬撑罢了！🥰人活着哪有不喜欢虹夏的，硬撑罢了！🥰人活着哪有不喜欢虹夏的，硬撑罢了！🥰人活着哪有不喜欢虹夏的，硬撑罢了！🥰人活着哪有不喜欢虹夏的，硬撑罢了！🥰人活着哪有不喜欢虹夏的，硬撑罢了！🥰人活着哪有不喜欢虹夏的，硬撑罢了！</p><p><img src="/2023/02/28/gossip-4/hongxia.jpeg"></p><h3 id="20230208"><a href="#20230208" class="headerlink" title="20230208"></a>20230208</h3><p>可是喜多也好可爱，怎么办。</p><h3 id="20230213"><a href="#20230213" class="headerlink" title="20230213"></a>20230213</h3><p>《别酱了》这动画……是给枫加了多少料啊？</p><h3 id="20230214"><a href="#20230214" class="headerlink" title="20230214"></a>20230214</h3><p>互联网真是太奇妙啦！无所不包，无所不有，真是什么逆天的玩意都能见着。虽然说进行这种垃圾转运工作只能恶心到更多的人，但毕竟是自己的站点，就算真有谁被转运来的垃圾恶心到了，也骂不了我。</p><p>当然还是要声明一下：本站绝非垃圾转运中心。</p><p><img src="/2023/02/28/gossip-4/nitian1.png"></p><p>特意把用户名截了。毕竟哥们不是来挂人的，只是倒个垃圾。</p><p>这位互联网大仙到底是用什么看的番啊😅。但凡用自己的眼睛和脑子看个一集，也知道它多媚宅了。没人来打物化女性的拳已经谢天谢地了，这大仙还用它反打女拳？</p><p>“其中的寓意不言自明”😋，人家就是画点TS百合的日常图一乐，还真给自己看魔怔了。哥们也就是来图一乐，不是来看别人发癫的。把这种莫名其妙的言论发到公共平台给别人看，真是不怕笑话。</p><p>不过话说回来，在互联网上冲浪，比起提防别人笑话，更重要的还是保护好自己的父母。</p><p><img src="/2023/02/28/gossip-4/nitian2.png"></p><p>本来被这大仙败了兴致，翻到底下回复又给我看乐了</p><h3 id="20230216"><a href="#20230216" class="headerlink" title="20230216"></a>20230216</h3><p>一口气看了25集《DEATH NOTE》，真的只能用神仙打架来形容。计划通！</p><p>但为什么25集就赢了L？后面12集在干嘛？</p><p>额，看到第27集，这俩小孩又是哪来的？前面完全没有交代，太唐突了。不会是为了结尾设计的最后贏家吧。</p><p>M这角色都是些什么剧情，看得真难受。N这角色开金手指了？坐那一想全是对的，他比L都牛逼。说话也太矫揉造作了。对这俩角色没什么好感。</p><p>这结局真逆天，一点……推理要素都没有了。一直都是老老实实按指示做事的，偏偏就在最关键的节点自作主张？我不理解好吧，我不理解。反正剧情要月输，N说啥就是啥呗。</p><p>26-37集真是狗尾续貂，和前面部分的质量差太远了。智斗成了飙直觉讲故事就没啥看头了。挂哥嘛，你拿什么和他拼？</p><p>通篇看下来还是夜神月和弥海砂两个角色让人印象深刻，毕竟是男女主角，人物塑造还是很深刻的。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>杂记 2023年1月辑</title>
      <link href="/2023/01/31/gossip-3/"/>
      <url>/2023/01/31/gossip-3/</url>
      
        <content type="html"><![CDATA[<h3 id="20230109"><a href="#20230109" class="headerlink" title="20230109"></a>20230109</h3><p>原来自己论文的进度这么落后吗……</p><h3 id="20230115"><a href="#20230115" class="headerlink" title="20230115"></a>20230115</h3><p>开摆😋！</p><h3 id="20230116"><a href="#20230116" class="headerlink" title="20230116"></a>20230116</h3><p>接着开摆😋！</p><h3 id="20230118"><a href="#20230118" class="headerlink" title="20230118"></a>20230118</h3><p>接着开摆😋！</p><h3 id="20230119"><a href="#20230119" class="headerlink" title="20230119"></a>20230119</h3><p>接着开摆😋！</p><h3 id="20230120"><a href="#20230120" class="headerlink" title="20230120"></a>20230120</h3><p>接着开摆😋！</p><h3 id="20230126"><a href="#20230126" class="headerlink" title="20230126"></a>20230126</h3><p>过年嘛，就该出去走走，在家休息，什么的。</p><h3 id="20230131"><a href="#20230131" class="headerlink" title="20230131"></a>20230131</h3><p>轻松的一月。但回头看真的如此吗？</p><p>新年已经过了十天了，要开始奋斗了捏😋！</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>杂记 2022年12月辑</title>
      <link href="/2022/12/31/gossip-2/"/>
      <url>/2022/12/31/gossip-2/</url>
      
        <content type="html"><![CDATA[<h3 id="20221203"><a href="#20221203" class="headerlink" title="20221203"></a>20221203</h3><p>18：00了，我晚饭呢？</p><h3 id="20221204"><a href="#20221204" class="headerlink" title="20221204"></a>20221204</h3><p>很没干劲啊，明明事情一堆的。</p><p>开摆，看《CLANNAD ～AFTER STORY～》去了。</p><p>第1话果然是温馨日常，麻枝准祖传的蜜汁打棒球剧情，不可不评鉴。</p><p>第2话是春原兄妹线了吗。春原，想不到你竟有枭雄之志……</p><p>第4话妹妹的操作说实话看不懂。蛮多槽点的。主要还是互相关心但不能互相理解的问题吗？</p><p>好兄弟啊只能说。是兄弟就来打我！</p><h3 id="20221212"><a href="#20221212" class="headerlink" title="20221212"></a>20221212</h3><p>啥都不想干……</p><h3 id="20221214"><a href="#20221214" class="headerlink" title="20221214"></a>20221214</h3><p>由于太久（三个月）没开机，有台电脑的密码给我忘了。想了下上面应该没有什么重要文件，索性重装了系统。Arch Linux，香！</p><h3 id="20221216"><a href="#20221216" class="headerlink" title="20221216"></a>20221216</h3><p>还是啥都不想干。每天都没有什么目标，就这样混过去。</p><p>给网站换了个背景图。中间那块总是会被挡住，不过既然是背景，也无所谓了。</p><h3 id="20221217"><a href="#20221217" class="headerlink" title="20221217"></a>20221217</h3><p>前两天说的那台电脑，上面只装Linux系统已经一年多了。虽然实际使用时间没有一年多，但大半年肯定是有了。</p><p>感觉Linux日用问题不大。至于工作学习需求，JetBrains全家桶应该都可以运行，至少Idea和Pycharm我都能用，轻量级的VS Code也很好用，实际上我写很多代码时用Anaconda自带的Jupyter Lab都够了。至于Windows专有的软件，Matlab什么的，我也不用，Adobe的Ps、Lr之类的也用不着了。可能最大的短板在玩游戏上？这样一想家长给自己的孩子装台Linux电脑岂不是……而且Linux对这台电脑的性能要求还低，跑起来比装Windows时快多了，毕竟只有8G内存（现在可是2022年了）。但我总觉得这台Linux电脑还能&#x2F;要再干些什么。</p><p>好吧拐弯抹角说了这么多其实我只是想知道Linux上该怎么看视觉小说。</p><h3 id="20221221"><a href="#20221221" class="headerlink" title="20221221"></a>20221221</h3><p>还是啥都不想干……继续开摆！</p><h3 id="20221224"><a href="#20221224" class="headerlink" title="20221224"></a>20221224</h3><p>WiFi突然断了，电脑还咋咋呼呼和我说什么受限的连接。一开始以为是电脑问题，后来发现什么设备连着都上不了网。路由器重启了也没用，搞了半天也没弄好。仔细一看光猫“光信号”那栏跳红灯，在想是不是线路断了，但真的会有这种事吗？</p><p>结果还真是。附近拆迁把这一片光缆弄断了，还没抢修好……</p><h3 id="20221227"><a href="#20221227" class="headerlink" title="20221227"></a>20221227</h3><p>连着看了Kancolle动画第二季的前三集。前两集看完我直呼好看，第三集看完我只能说6。</p><p>我对动画走不走史实倒是无所谓，但这飞天舰娘……好吧就算退一万步讲她能起飞，但这一个战列舰踩着驱逐舰腾空……我……</p><p>我真的无话可说，我看不懂但我大受震撼。</p><p>去看了下第三集评论，果然，和山城一并起飞成功的还有田中的🐎。</p><p>话说你既然都能飞起来了第二话连个鱼雷都扭不开？？？</p><p><img src="/2022/12/31/gossip-2/2022-12-27-223322.png" alt="这就是航空战列舰吗，长见识了"></p><h3 id="20221229"><a href="#20221229" class="headerlink" title="20221229"></a>20221229</h3><p>我以为Kancolle第二季第3话已经是想象力的巅峰了，直到我看了第4话。</p><h3 id="20221231"><a href="#20221231" class="headerlink" title="20221231"></a>20221231</h3><p>一天速成报告，我急了。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux也要看视觉小说！</title>
      <link href="/2022/12/18/visual-novel/"/>
      <url>/2022/12/18/visual-novel/</url>
      
        <content type="html"><![CDATA[<blockquote><p>——你为什么要攀登珠峰？</p><p>——因为山就在那里。</p></blockquote><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>突然想看视觉小说。众所周知，Windows是浏览视觉小说的最佳方式，但我现在用的电脑装的是Linux，exe程序是跑不起来的。这就很苦恼。</p><p>如果看到这里的你不明所以，问我视觉小说到底是什么？我也不好说，这就是一种分类，有很多别称，你也可以叫它文字冒险游戏（ADV）等等。当然这些名称的具体含义实际上多少有些差别，但怎么叫都无所谓，只是我习惯叫这个罢了。</p><p>言归正传，Linux到底能不能跑视觉小说？经过我一天的研究，得出的结论是：能跑，但只能跑一点点。😥</p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>1.装虚拟机+Windows。我没试过。</p><p>2.ONScripter类模拟器+ONS移植游戏。试了一下，我的评价是PC端还是别用了。</p><p>3.使用wine。</p><p>本机使用的Linux发行版为Arch Linux，所以使用pacman包管理，其他发行版改下命令应该就行了。安装wine-staging版以及相关库（64位以及32位）。我也不知道这些库是否必装，如果想节省空间可以先只装wine。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo pacman -S wine-mono wine_gecko wine-staging giflib lib32-giflib libpng lib32-libpng libldap lib32-libldap gnutls lib32-gnutls mpg123 lib32-mpg123 openal lib32-openal v4l-utils lib32-v4l-utils libpulse lib32-libpulse libgpg-error lib32-libgpg-error alsa-plugins lib32-alsa-plugins alsa-lib lib32-alsa-lib libjpeg-turbo lib32-libjpeg-turbo sqlite lib32-sqlite libxcomposite lib32-libxcomposite libxinerama lib32-libgcrypt libgcrypt lib32-libxinerama ncurses lib32-ncurses opencl-icd-loader lib32-opencl-icd-loader libxslt lib32-libxslt libva lib32-libva gtk3 lib32-gtk3 gst-plugins-base-libs lib32-gst-plugins-base-libs vulkan-icd-loader lib32-vulkan-icd-loader</span><br></pre></td></tr></table></figure><p>我安装的版本信息如下（使用sudo pacman -Qs ^wine-命令可查看）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">local/wine-gecko 2.47.3-1</span><br><span class="line">    Wine&#x27;s built-in replacement for Microsoft&#x27;s Internet Explorer</span><br><span class="line">local/wine-mono 7.4.0-1</span><br><span class="line">    Wine&#x27;s built-in replacement for Microsoft&#x27;s .NET Framework</span><br><span class="line">local/wine-staging 7.22-1</span><br><span class="line">    A compatibility layer for running Windows programs - Staging branch</span><br></pre></td></tr></table></figure><p>按道理来说，初次winecfg设置自动生成的64位环境应该没啥问题。但我后面使用winetricks老是弹warning（真的是弹窗warning），想了想还是将环境改为win32位。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 如果已经生成64位环境，用以下命令删除</span><br><span class="line">rm -rf $HOME/.wine &amp;&amp; rm -f $HOME/.config/menus/applications-merged/wine* &amp;&amp; rm -rf $HOME/.local/share/applications/wine &amp;&amp; rm -f $HOME/.local/share/desktop-directories/wine*</span><br><span class="line"># 设置为32位环境</span><br><span class="line">export WINEARCH=win32</span><br><span class="line"># 在初始化一下wine</span><br><span class="line">winecfg</span><br></pre></td></tr></table></figure><p>安装winetricks，为了GUI界面可能还要装个kdialog（看安装后运行winetricks返回的提示）。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo pacman -S winetricks</span><br></pre></td></tr></table></figure><p>使用winetricks进行依赖安装。说实话我也不知道要装哪些。我安装的依赖清单如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ winetricks list-installed</span><br><span class="line">------------------------------------------------------</span><br><span class="line">warning: Github down? version &#x27;&#x27; doesn&#x27;t appear to be a valid version</span><br><span class="line">------------------------------------------------------</span><br><span class="line">Executing mkdir -p /home/***</span><br><span class="line">Using winetricks 20220411 - sha256sum: ****************************** with wine-7.22 (Staging) and WINEARCH=win32</span><br><span class="line">sourcehansans</span><br><span class="line">fakechinese</span><br><span class="line">wsh57</span><br><span class="line">gdiplus</span><br><span class="line">directx9</span><br><span class="line">d3dx9</span><br><span class="line">quartz</span><br><span class="line">devenum</span><br><span class="line">wmp11</span><br><span class="line">w_workaround_wine_bug-34803</span><br><span class="line">remove_mono</span><br><span class="line">winxp</span><br><span class="line">vcrun6</span><br><span class="line">wmv9vcm</span><br><span class="line">ole32</span><br><span class="line">atmlib</span><br><span class="line">fakejapanese</span><br></pre></td></tr></table></figure><p>按道理来说这应该就好了。</p><p>可当我兴冲冲地下载minori的SPPL，在终端输入wine sppl1.exe命令时，它给我弹了个什么都没有的窗口，然后在Document文件夹里生成了个minori文件夹之后……</p><p>就什么动静都没有了。</p><p>我反复试了几次，都是不能打开，我也不知道到底缺了什么依赖文件，终端log更是啥也没说。很难绷得住。</p><p>于是心灰意冷的我又随便找了个游戏下载，再次打开，神奇的事情发生了：</p><p>程序运行了。没错，虽然标题栏顶着一堆乱码，菜单栏也是一堆方块（应该是少了字体），但确实能打开。进入游戏内，除了文本大小不匹配导致只能显示上半边（但居然能看出是哪些字，汉字，很神奇吧），其他都正常。</p><p>于是我想，会不会是老游戏运行成功率比较高呢？毕竟我设置的wine环境是32位win7。</p><p>然后我打开了琥珀结晶——才怪，根本打不开。</p><p>到这里我已经开始想今天都干了些啥了，但我就很好奇难道装了这么多依赖就没有一个能正常运行的程序了吗？</p><p>还真让我找到了：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">WINEARCH=win32 WINEVER=win7 wine RiddleJoker_CHS.exe</span><br></pre></td></tr></table></figure><p><img src="/2022/12/18/visual-novel/Screenshot_20221218_163211.png"></p><p>居然一切正常，我直接哭出来😭！谢谢你，Yuzusoft。今天，我是柚子厨！</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Linux+wine，表现只能算一般。有的能运行，有的不能运行，这下不是人选游戏，反而成电脑选游戏了，这不好。至于Android、IOS端用模拟器玩，也是一个道理。当然wine不是模拟器。现在想想折腾wine一天的时间，装个虚拟机早就装好了。想看视觉小说，还是Windows解千愁。</p>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>杂记 2022年11月辑</title>
      <link href="/2022/11/30/gossip-1/"/>
      <url>/2022/11/30/gossip-1/</url>
      
        <content type="html"><![CDATA[<p>开始碎碎念……</p><h3 id="20221101"><a href="#20221101" class="headerlink" title="20221101"></a>20221101</h3><p>偶尔也得写点东西，免得有人说我不更新。大家都是读书人，不更新的事，叫忙，怎么能说是咕了呢？</p><p>非要说是我咕了，也不是不行，但我也要辩解一番。之前写的Spark记录，多好啊，大伙却不爱看，不看就不看咯，还说没意思。<strong>我就想不明白，Spark怎么就没意思了？</strong></p><p>于是不敢轻易动笔，除非写点有意思的东西。但写什么才叫有意思呢，我也不知道。这样一来，不就咕了嘛😋。</p><p>不过说了这么多废话，我还是不知道写什么。既然如此，那就先写到这里吧。</p><p>想了想这篇文章干脆就拿来记些零零碎碎的东西吧。</p><p>今天听ClariS的歌，还蛮好听的，虽然听不懂歌词在唱什么就是了。但也无伤大雅，毕竟不是诗歌鉴赏，何必去深究歌词含义呢。可能自己听歌时的所思所想与歌词真意并不一样，然对于自己而言，耳有所闻、心有所感，亦已足矣。人的想法，独处与群聚会变化，此时与彼时会变化，此处与彼处亦会变化，至少我现在一个人在房间听这歌的时候，是这么想、这么感受的。</p><h3 id="20221108"><a href="#20221108" class="headerlink" title="20221108"></a>20221108</h3><p>一直在摆烂，摆烂的间隙痛骂自己，骂完接着摆烂。虽然有很多事没做，也不知道干什么。如果尽是摆烂的话，只能说每天过的都一模一样，不知何日。</p><p>不过今天发现了点不同，路上的大家都站着看天空，一看确实是个红月亮，还听说是月食。但拿起手机准备拍照，却发现拍出来和个路灯似的，便也不想拍了。如此又是摆烂的一天。</p><p>话说回来这种根本不会改正的自我反思，一来无所意义，二来为人笑柄，为什么还是要发出来丢人呢，我想一定是为了与看到这里的摆烂哥共勉吧。</p><p>《DESIRE》不管什么时候听，都很好听啊。</p><p>今天AMD Radeon Software突然打不开了，但没有其他任何问题，不过强迫症犯了还是去电脑官网找了最新核显驱动，把驱动重装了。重装完这软件就能打开了。结果现在过了好一会打开任务管理器发现里面独显的编号变成了GPU-0，核显的反而成了GPU-1。更抽象的是C盘和D盘的编号也换了，D盘成了磁盘0，C盘成了磁盘1……希望别整了什么坑吧。</p><h3 id="20221109"><a href="#20221109" class="headerlink" title="20221109"></a>20221109</h3><p>哈哈，AMD Radeon Software这软件又打不开了。</p><h3 id="20221110"><a href="#20221110" class="headerlink" title="20221110"></a>20221110</h3><p>今天右键突然发现快捷菜单多了个AMD Link For Windows，一看AMD Radeon Software那玩意又能打开了。它总是能给我整点新花样啊，不过已经可以了，就到这里吧。</p><p>听歌听到AIR OST中的一首，很惆怅，就算不去想故事的剧情，只听这段旋律，也足以让人茕独而有怀矣。无怪古人以礼乐治国。乐也者，情之不可变者也；礼也者，理之不可易者也。乐统同，礼辨异，礼乐之说，管乎人情矣。音乐着实是人类情感强烈而真实的表达。更无怪季札观周乐，能一言中的。</p><h3 id="20221112"><a href="#20221112" class="headerlink" title="20221112"></a>20221112</h3><p>公主连结Re:Dive意外地好看啊，超过我的预期。</p><p>第二季爷终于会说话了，了不起！</p><h3 id="20221113"><a href="#20221113" class="headerlink" title="20221113"></a>20221113</h3><p>看完第11集，好感动，果然爷跨越了一次次轮回，一定是为了能够与霸瞳重逢吧？</p><p>看到第12集了，真不愧叫公主连结。你问我主角是谁？我不好说，不过应该不是爷。</p><p>这12集看到一半，爷居然冲上去了，还一下就翻盘了，什么叫公主骑士啊。</p><p>怎么爷刚被刀过就又活了，还终于讲了两句帅台词。这什么剧情，反转又反转是吧。</p><p>反转了，爷就好这种剧情。</p><p>看完了。大家的表现都很不错，希望不要在游戏里演我。</p><p><img src="/2022/11/30/gossip-1/20221113040934.png"></p><p>霸瞳，太优雅了霸瞳😭👍！</p><h3 id="20221120"><a href="#20221120" class="headerlink" title="20221120"></a>20221120</h3><p>突然想体验一下极简风格，换了一个主题。</p><p>然后调了半天设置。</p><p>说真的一开始想的是把Wordpress改成Typecho，用后者的默认主题。但得重装服务器，好麻烦。</p><p>人是有惰性的，至少我是如此。</p><h3 id="20221121"><a href="#20221121" class="headerlink" title="20221121"></a>20221121</h3><p>校园网下载速度飙到了60多m&#x2F;s，甚至70多m&#x2F;s。</p><p>明明之前一直600kb&#x2F;s的下载速度，这下彻底疯狂了。只求待会儿网管别找上门来。</p><p>后续：速度降到了77kb&#x2F;s，对味了。校园网啊，就该这样。不过连刚才600kb&#x2F;s的待遇也不能享有了，怕不是已经被制裁了。</p><p>后续的后续：</p><p><img src="/2022/11/30/gossip-1/2022-11-21-230331.png"></p><p>啊？</p><h3 id="20221123"><a href="#20221123" class="headerlink" title="20221123"></a>20221123</h3><p>Tomoyo After 太好玩啦！</p><p>……</p><p>通关了……只能说被拿下了。</p><h3 id="20221127"><a href="#20221127" class="headerlink" title="20221127"></a>20221127</h3><p>决定再看一遍CLANNAD。</p><p>欢乐的日常，好看😋！</p><p>才到第7集啊，就有点……</p><p>第8集依旧稳定发挥。</p><p>第9集，我的超人😭！</p><p>温馨、落寞、伤感。就像……冬天里的一丝微温？</p><p><img src="/2022/11/30/gossip-1/2022-11-27-235048.png"></p><p><img src="/2022/11/30/gossip-1/2022-11-27-235104.png"></p><p>每次背景的旋律响起，再一转这个镜头，都让我不禁落下泪来。非常的震撼！</p><p><img src="/2022/11/30/gossip-1/2022-11-28-000804.png"></p><p>😭好！</p><p>第一次看的时候就哭了。没想到隔了几年再看，依旧这么的打动人心。</p><h3 id="20221128"><a href="#20221128" class="headerlink" title="20221128"></a>20221128</h3><p>第10集开始琴美线了。琴美可爱捏。高双马尾真好看啊，让人想到阿喵喵。</p><p>第11话名场面。</p><p><img src="/2022/11/30/gossip-1/2022-11-28-014233.png"></p><p><strong><em>前天我遇见了小兔，昨天遇见了小鹿，今天则遇见了你。</em></strong></p><p>第12话又开始发力了。天降青梅，我的超人！</p><p><img src="/2022/11/30/gossip-1/2022-11-28-145911.png"></p><p>第14话，琴美线应该告一段落了。感动。琴美线也是我玩游戏时很喜欢的一条故事线。人物也好、剧情也好，都写的很不错。尤其是朋也想起过去之时，《町、時の流れ、人》随之响起，有一种震慑心灵的感觉。特别是戴着耳机，那种环绕的感觉，让人不禁战栗起来……</p><p>话说这首BGM出场率蛮高啊。朋也在晚上跑出家门，听到渚对他说“_<strong>如果可以的话，请让我带你去吧，在这个小镇，实现愿望的地方…</strong>_”之时，好像也是这首歌吧？</p><p>第15话开始是谁的线了呢？真是一开始看CLANNAD就停不下来了。但再不准备组会的东西今晚就要寄了……（没忍住，接着看吧。彻底开摆！）</p><p>我擦，原来是智代线吗？</p><p>智代我的智代！😭😭😭人哪有不喜欢智代的？硬撑罢了！</p><p>哈哈，直接到家里叫人起床上学也太搞了。话说回来关于朋也与父亲的和解，CLANNAD After Story和Tomoyo After感觉完全两样呢。</p><p>这第18话后半部分也太强力了。四杀……</p><p>第22话看的好紧张。大叔，你太帅了！😭</p><p>感动！果真治愈。</p><p><img src="/2022/11/30/gossip-1/2022-11-29-001238.png"></p><p>番外？这是好文明。</p><p>看完了，满足怎么说满足。</p><p>话说要接着看CLANNAD After Story吗？总有点没准备好的感觉。</p><p>先看下智代和杏的OVA吧。</p><h3 id="20221129"><a href="#20221129" class="headerlink" title="20221129"></a>20221129</h3><p>看完智代线了。想到前阵子刚玩的《智代After》，真是百感交集……</p><p>真喜欢杏的性格啊。总觉得杏和智代很像，不是吗？</p><p>都说当断则断，可真的有那么容易吗。虽然我不会有这种烦恼就是了。话说春原真是情商拉满了啊。</p><p><img src="/2022/11/30/gossip-1/2022-11-29-012413.png"></p><p><img src="/2022/11/30/gossip-1/2022-11-29-014147.png"></p><p>雪中智代雨中杏</p><p>感慨。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>他多少还是有点懂的</title>
      <link href="/2022/11/11/kaguya/"/>
      <url>/2022/11/11/kaguya/</url>
      
        <content type="html"><![CDATA[<p>忽闻辉夜姬漫画已完结，连夜看完。</p><p>所以现在是11号凌晨，或许也能说是10号？应该不行，还是得算11号。大半夜的头脑也发昏了吗，绝对是看这漫画的缘故。光棍节看恋爱喜剧，岂不美哉，岂不妙哉？真不失为人生一大乐事😭。</p><p>上次看的时候，还只是炒面party那几话。</p><p>说回结局。</p><p>至少每个人都写了结局，不也挺好的吗😋？这就是所谓的Happy Ending吧。</p><p>怎么说呢，百感交集，思绪万千，却又如鲠在喉，不知何言。小说也好，电影也好，漫画也好游戏也好，每当故事结局的时候，总是令人惆怅。曲终人散之感即如是乎？</p><blockquote><p>带给我极大的震撼，看完后我兀坐在试片间内，久久不得动弹，也不愿出去，连看两场。</p><p>我看不懂，但我大受震撼。</p></blockquote><p>最开始看到这部漫画，着实令我震撼而惊叹，如今看到完结篇，也予我一记强烈冲击。</p><p>冷静下来想想，也不觉得有多离谱。真要我说的话，中国人向来喜欢大团圆的结局，所以这种结局画的好。而中国人向来也是讲究含蓄的，话通常不能说得太直白。我只能说，赤坂是懂漫画的，不过这么好的功底，不去打游戏而在这里画漫画，真的是可惜！</p><p>这就说完了？怎么可能。</p><p><img src="/2022/11/11/kaguya/2022-11-11-032409.png"></p><p>我早坂呢？我早坂呢？</p><p><img src="/2022/11/11/kaguya/2022-11-11-032731.png"></p><p>早坂，太可爱啦早坂！😇🙏</p><p>可为啥只有你的结局是在做梦啊？😭😭😭</p><p>我要化身乐子人，疯狂地温习《辉夜姬想让人告白 同人版》第32话！</p><p>深夜发病，自娱之语，图一笑耳，妄言妄听。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习记录（七）：Spark安装</title>
      <link href="/2022/09/16/spark7/"/>
      <url>/2022/09/16/spark7/</url>
      
        <content type="html"><![CDATA[<p>本来以为这个系列应该就不更新了，毕竟也没人看🤣。其实主要是学到Spark那里时忘记做记录了，装好了之后也不知道有啥要记录的了。</p><p>不过想不到最近又要安装Spark系统了，但是也懒得一张张截图记录步骤了，就直接贴个自己看的教程😋。也顺便做个备份（上次装好就把教程删了，这回找了好久😥）。</p><p>因为之前装了Hadoop，即Spark是安装在之前Hadoop的基础上的，所以在下载安装包时要选择Pre-build with user-provided Hadoop，也就是像spark-3.2.1-bin-without-hadoop.tgz这种。</p><p>下面是Local模式（单机模式）的 Spark安装流程。（默认用户为hadoop）。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo tar -zxf ~/Download/spark-3.2.1-bin-without-hadoop.tgz -C /usr/local</span><br><span class="line">cd /usr/local</span><br><span class="line">sudo mv ./spark-3.2.1-bin-without-hadoop/ ./spark</span><br><span class="line">sudo chown -R hadoop:hadoop ./spark</span><br></pre></td></tr></table></figure><p>然后修改配置文件。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/spark</span><br><span class="line">cp ./conf/spark-env.sh.template ./conf/spark-env.sh</span><br><span class="line">vim ./conf/spark-env.sh</span><br></pre></td></tr></table></figure><p>添加如下配置，使Spark可以读取HDFS数据：</p><p>export SPARK_DIST_CLASSPATH&#x3D;$(&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;bin&#x2F;hadoop classpath)</p><p>当然这种安装方法是需要先启动Hadoop服务再启动Spark的。</p><p>可以通过自带示例检查安装是否成功：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/spark</span><br><span class="line">bin/run-example SparkPi</span><br></pre></td></tr></table></figure><p>Spark可以通过spark-shell命令启动，或是使用pyspark。可在~&#x2F;.bashrc文件中加入SPARK_HOME和PATH变量，使得相关命令可直接在终端中运行。</p>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习记录（六）：HBase安装</title>
      <link href="/2022/03/28/spark6/"/>
      <url>/2022/03/28/spark6/</url>
      
        <content type="html"><![CDATA[<p>按照本次学习的教程，之后是进行HBase的安装和配置。（怎么感觉这一系列的名称应该改成“Hadoop学习记录”？好奇怪。🤔）</p><p>和之前各软件的安装一样，进入HBase安装包的下载路径，打开终端输入：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo tar -zxf hbase-2.4.10-bin.tar.gz -C /usr/local</span><br><span class="line">sudo mv /usr/local/hbase-2.4.10 /usr/local/hbase</span><br></pre></td></tr></table></figure><p>再修改环境变量（vim ~&#x2F;.bashrc修改，source ~&#x2F;.bashrc更新），添加以下内容：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#hbase</span><br><span class="line">export PATH=$PATH:/usr/local/hbase/bin</span><br></pre></td></tr></table></figure><p>赋予可执行权限（终端中输入）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local</span><br><span class="line">sudo chown -R hadoop ./hbase</span><br></pre></td></tr></table></figure><p>接下来可以先检查安装是否完成。切换回主目录（cd ~），在终端中输入：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase version</span><br></pre></td></tr></table></figure><p>这时返回以下结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/usr/local/hbase/lib/client-facing-thirdparty/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]</span><br><span class="line">HBase 2.4.10</span><br><span class="line">Source code repository git://buildbox/home/apurtell/build/hbase revision=3e5359c73d1a96dd7d2ac5bc8f987e9a89ef90ea</span><br><span class="line">Compiled by apurtell on Mon Feb 28 10:03:15 PST 2022</span><br><span class="line">From source with checksum b38d895e719d82c3d3b2a886e3fca39f754f5ceef732e157604c905c7af3af2d4b9b861abfdd270e627a4fe1ffa8eba74a9b01095b856952d18023666cce66ad</span><br></pre></td></tr></table></figure><p>好像有点奇怪？看下面能成功返回版本信息，但这上面几行是怎么回事。可怕😰。（注：之后解决了这个问题。这一点将会写在文末。）</p><p>接着开始配置HBase。HBase同样有多种配置方式，本次也采用伪分布式配置。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /usr/local/hbase/conf/hbase-env.sh</span><br></pre></td></tr></table></figure><p>修改以下几项：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/lib/jdk1.8.0_301</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HBASE_CLASSPATH=/usr/local/hadoop/conf</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HBASE_MANAGES_ZK=true</span><br></pre></td></tr></table></figure><p>接着修改另一项文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /usr/local/hbase/conf/hbase-site.xml</span><br></pre></td></tr></table></figure><p>找到：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.tmp.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;./tmp&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>改为:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.tmp.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;./tmp&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.rootdir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs://localhost:9000/hbase&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>至此配置也完成了。下面将进行检验。</p><p>打开终端依次输入：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh localhost</span><br><span class="line">start-dfs.sh</span><br><span class="line">jps</span><br></pre></td></tr></table></figure><p>看到以下进程，说明Hadoop启动成功：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">8069 Jps</span><br><span class="line">7959 SecondaryNameNode</span><br><span class="line">7580 NameNode</span><br><span class="line">7724 DataNode</span><br></pre></td></tr></table></figure><p>启动HBase进程：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-hbase.sh</span><br></pre></td></tr></table></figure><p>再用jps命令查看，出现下列进程说明启动成功：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">8592 HMaster</span><br><span class="line">8770 HRegionServer</span><br><span class="line">7959 SecondaryNameNode</span><br><span class="line">8471 HQuorumPeer</span><br><span class="line">9114 Jps</span><br><span class="line">7580 NameNode</span><br><span class="line">7724 DataNode</span><br></pre></td></tr></table></figure><p>接着就可以启动HBase的shell进行操作了。在终端中输入（exit可退出shell）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase shell</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/usr/local/hbase/lib/client-facing-thirdparty/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]</span><br><span class="line">HBase Shell</span><br><span class="line">Use &quot;help&quot; to get list of supported commands.</span><br><span class="line">Use &quot;exit&quot; to quit this interactive shell.</span><br><span class="line">For Reference, please visit: http://hbase.apache.org/2.0/book.html#shell</span><br><span class="line">Version 2.4.10, r3e5359c73d1a96dd7d2ac5bc8f987e9a89ef90ea, Mon Feb 28 10:03:15 PST 2022</span><br><span class="line">Took 0.0012 seconds                                                                                                                                                                                                                                                               </span><br><span class="line">hbase:001:0&gt;</span><br></pre></td></tr></table></figure><p>怎么又是这串warning？哈人😨。但又能成功启动，好怪啊。（注：<strong>之后解决了这个问题。这一点将会写在文末</strong>。）</p><p>要关闭HBase，在终端中输入：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stop-hbase.sh</span><br></pre></td></tr></table></figure><p>不过，在等待了很久后，HBase还是没关掉（已经不知道几个点了）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stopping hbase...............................................................................................................................................................................</span><br></pre></td></tr></table></figure><p>搜索得知，可以这样关闭。先在终端中输入：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase-daemon.sh stop master</span><br></pre></td></tr></table></figure><p>再输入：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stop-hbase.sh</span><br></pre></td></tr></table></figure><p>真🐂啊，一下就关掉了。不过这样真的没问题？（还真挖了个坑，详见下文。）</p><p>然后是关闭Hadoop进程与退出本机ssh连接：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#close the hadoop</span><br><span class="line">stop-dfs.sh</span><br><span class="line">#close the ssh</span><br><span class="line">exit</span><br></pre></td></tr></table></figure><p>安装和配置就到此完成了……吗？没有，还记得上面两大段warning吗？通过warning本身提供的信息（就是那个网址），可以得知：</p><p><img src="/2022/03/28/spark6/2022-03-28-121017.png"></p><p><img src="/2022/03/28/spark6/2022-03-28-121214.png"></p><p>大意就是Hadoop和HBase都调用了这个API。并且虽然你会看到warning，但程序会自动选择其中一个运行，并不会影响整体的运行，当然你也可以去掉其中一个就不会出现这种提醒了。</p><p>但我哪里敢乱删文件啊，每次看着这一长串又有点烦😢。于是我后来又检查了一遍HBase的环境变量配置文件（vim &#x2F;usr&#x2F;local&#x2F;hbase&#x2F;conf&#x2F;hbase-env.sh），发现只要不让HBase再调用一遍Hadoop的库好像就行了，也就是修改这一项：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Tell HBase whether it should include Hadoop&#x27;s lib when start up,</span><br><span class="line"># the default value is false,means that includes Hadoop&#x27;s lib.</span><br><span class="line">export HBASE_DISABLE_HADOOP_CLASSPATH_LOOKUP=&quot;true&quot;</span><br></pre></td></tr></table></figure><p>再启动一次HBase试试：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br><span class="line">start-hbase.sh</span><br></pre></td></tr></table></figure><p>然后我看到了这串输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1: running zookeeper, logging to /usr/local/hbase/bin/../logs/hbase-hadoop-zookeeper-ubuntu.out</span><br><span class="line">running master, logging to /usr/local/hbase/bin/../logs/hbase-hadoop-master-ubuntu.out</span><br><span class="line">: regionserver running as process 8770. Stop it first.</span><br></pre></td></tr></table></figure><p>啊这，怎么回事呢😦？</p><p>输入jps命令，发现目前运行的进程是这几个：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">13427 Jps</span><br><span class="line">13109 DataNode</span><br><span class="line">13318 SecondaryNameNode</span><br><span class="line">11831 HQuorumPeer</span><br><span class="line">12927 NameNode</span><br></pre></td></tr></table></figure><p>还有这事？于是再输入一遍stop-hbase.sh命令，返回输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">no hbase master found</span><br><span class="line">127.0.0.1: running zookeeper, logging to /usr/local/hbase/bin/../logs/hbase-hadoop-zookeeper-ubuntu.out</span><br><span class="line">127.0.0.1: stopping zookeeper.</span><br></pre></td></tr></table></figure><p>此时jps命令返回正常，只有Hadoop相关进程：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">13109 DataNode</span><br><span class="line">13318 SecondaryNameNode</span><br><span class="line">12927 NameNode</span><br><span class="line">13711 Jps</span><br></pre></td></tr></table></figure><p>这时我注意到了输出中的“no hbase master found”。啊这。再输入start-hbase.sh命令重新启动后，这次没有“Stop it first.”了。</p><p>不过检查jps进程发现，确实少了个HMaster。我的master呢？你就是我的Master吗？</p><p>于是照着之前关闭时的命令输入了：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase-daemon.sh start master</span><br></pre></td></tr></table></figure><p>再输入jps检查，进程终于正常了。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">14851 Jps</span><br><span class="line">13109 DataNode</span><br><span class="line">13318 SecondaryNameNode</span><br><span class="line">14633 HMaster</span><br><span class="line">13997 HQuorumPeer</span><br><span class="line">14238 HRegionServer</span><br><span class="line">12927 NameNode</span><br></pre></td></tr></table></figure><p>这时再启动hbase shell，就没有之前那一长串warning一样的信息了（不过又出来一句新的）。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">2022-03-28 12:18:51,591 WARN  [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">HBase Shell</span><br><span class="line">Use &quot;help&quot; to get list of supported commands.</span><br><span class="line">Use &quot;exit&quot; to quit this interactive shell.</span><br><span class="line">For Reference, please visit: http://hbase.apache.org/2.0/book.html#shell</span><br><span class="line">Version 2.4.10, r3e5359c73d1a96dd7d2ac5bc8f987e9a89ef90ea, Mon Feb 28 10:03:15 PST 2022</span><br><span class="line">Took 0.0013 seconds                                                                                                                                                                                                                                                               </span><br><span class="line">hbase:001:0&gt;</span><br></pre></td></tr></table></figure><p>此时退出shell，再输入hbase version命令，也没有那串warning了。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">HBase 2.4.10</span><br><span class="line">Source code repository git://buildbox/home/apurtell/build/hbase revision=3e5359c73d1a96dd7d2ac5bc8f987e9a89ef90ea</span><br><span class="line">Compiled by apurtell on Mon Feb 28 10:03:15 PST 2022</span><br><span class="line">From source with checksum b38d895e719d82c3d3b2a886e3fca39f754f5ceef732e157604c905c7af3af2d4b9b861abfdd270e627a4fe1ffa8eba74a9b01095b856952d18023666cce66ad</span><br></pre></td></tr></table></figure><p>之后正常退出HBase（这次就很快了）和Hadoop即可。本次安装和配置终于完成了😋。</p><p>但看着那句新的WARN消息，总觉得不对劲啊，有没有办法去掉它呢？🤔（解决了，方法见下文。）</p><p>附：<strong>关于“WARN [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable”消息的解决方案</strong>。</p><p>这个消息是HBase不能加载到Hadoop的原生库导致的，可能是上文将HBase的环境变量设置为不调用Hadoop的库所致。那到底调用不调用呢，好复杂😥。</p><p>但看官方文档<a href="https://hbase.apache.org/book.html#hadoop.native.lib">Apache HBase ™ Reference Guide</a>的说明：</p><blockquote><p>Let’s presume your Hadoop shipped with a native library that suits the platform you are running HBase on. To check if the Hadoop native library is available to HBase, run the following tool (available in Hadoop 2.1 and greater):</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/hbase --config ~/conf_hbase org.apache.hadoop.util.NativeLibraryChecker</span><br><span class="line">2014-08-26 13:15:38,717 WARN  [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Native library checking:</span><br><span class="line">hadoop: false</span><br><span class="line">zlib:   false</span><br><span class="line">snappy: false</span><br><span class="line">lz4:    false</span><br><span class="line">bzip2:  false</span><br><span class="line">2014-08-26 13:15:38,863 INFO  [main] util.ExitUtil: Exiting with status 1</span><br></pre></td></tr></table></figure><blockquote><p>Above shows that the native hadoop library is not available in HBase context.</p></blockquote><p>大意就是说，可以先通过命令检查Hadoop的native libray是否存在，如果检查结果都是false，那就说明是原生库的问题。但本次自己检查没有发现这个问题，故不再记录这一问题的解决方案了。</p><p>（注：搜索他人解决方案时看到，hadoop checknative -a命令也能起到检查效果。）</p><p>那么原生库存在的情况下，需要怎样解决呢？</p><blockquote><p>To fix the above, either copy the Hadoop native libraries local or symlink to them if the Hadoop and HBase stalls are adjacent in the filesystem. You could also point at their location by setting the environment variable in your hbase-env.sh.LD_LIBRARY_PATH</p></blockquote><p>大意就是说，**<em>需要在hbase-env.sh文件中增加LD_LIBRARY_PATH环境变量，值为Hadoop原生库的路径</em>**<em>（本文是 export LD_LIBRARY_PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;lib&#x2F;native ）。</em></p><p>之后重启HBase，再输入hbase shell命令，终端返回如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">~$ hbase shell</span><br><span class="line">HBase Shell</span><br><span class="line">Use &quot;help&quot; to get list of supported commands.</span><br><span class="line">Use &quot;exit&quot; to quit this interactive shell.</span><br><span class="line">For Reference, please visit: http://hbase.apache.org/2.0/book.html#shell</span><br><span class="line">Version 2.4.10, r3e5359c73d1a96dd7d2ac5bc8f987e9a89ef90ea, Mon Feb 28 10:03:15 PST 2022</span><br><span class="line">Took 0.0013 seconds                                                                                                                                                                                                                    </span><br><span class="line">hbase:001:0&gt;</span><br></pre></td></tr></table></figure><p>WARN消息没了，问题成功解决😋。</p>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习记录（五）：Hadoop安装之二</title>
      <link href="/2022/03/23/spark5/"/>
      <url>/2022/03/23/spark5/</url>
      
        <content type="html"><![CDATA[<p>在成功安装Hadoop后，还需要进行正确的配置才能投入使用。Hadoop的配置基本包括以下三种：单机配置（非分布式）、单节点上的伪分布式配置和分布式集群配置。单机配置就是Hadoop安装后默认的配置，即如上文安装后直接使用即可。但本次学习将进行单机伪分布式的配置。</p><p>参考教程，Hadoop 的配置文件位于&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;etc&#x2F;hadoop&#x2F;中，伪分布式需要修改2个配置文件：core-site.xml和hdfs-site.xml。Hadoop的配置文件是xml格式，每个配置以声明property的name和value的方式来实现。下面是具体的配置操作。</p><p>在终端中输入如下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop/etc/hadoop</span><br><span class="line">gedit core-site.xml</span><br></pre></td></tr></table></figure><p>正常情况下，打开后原配置文件应该是这样的内容：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>将其改为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>修改好后保存，继续修改另一项。在终端中输入：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gedit hdfs-site.xml</span><br></pre></td></tr></table></figure><p>将内容改为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.repliction&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>接着切换回Hadoop主路径，格式化修改好的配置即可：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop</span><br><span class="line">./bin/hdfs namenode -format</span><br></pre></td></tr></table></figure><p>正常情况下终端会输出一长串信息，主要看有没有ERROR，如果没有ERROR并且输出中显示已成功，说明之前的配置没有问题。成功的话一般会有提示（例：Storage directory &#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;tmp&#x2F;dfs&#x2F;name has been successfully formatted.）。</p><p>接下来就是启动进程，在终端中输入（仍然是&#x2F;usr&#x2F;local&#x2F;hadoop路径下）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-dfs.sh</span><br></pre></td></tr></table></figure><p>Hadoop启动😆！真顺利啊😋。</p><p>正当我这么想的时候，输出报错了😨：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Starting namenodes on [localhost]</span><br><span class="line">localhost: ERROR: JAVA_HOME is not set and could not be found.</span><br><span class="line">Starting datanodes</span><br><span class="line">localhost: ERROR: JAVA_HOME is not set and could not be found.</span><br><span class="line">Starting secondary namenodes [ubuntu]</span><br><span class="line">ubuntu: ERROR: JAVA_HOME is not set and could not be found.</span><br></pre></td></tr></table></figure><p>怎么回事，我不是在~&#x2F;.bashrc文件里配置过Java环境变量了吗😰？</p><p>于是开始检查，输入</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo $JAVA_HOME</span><br></pre></td></tr></table></figure><p>返回结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/lib/jdk1.8.0_301</span><br></pre></td></tr></table></figure><p>啊这🤡。于是又回去查教程，教程叫我修改Hadoop的环境变量文件。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop/etc/hadoop</span><br><span class="line">vim hadoop-env.sh</span><br></pre></td></tr></table></figure><p>将里面有一行# export JAVA_HOME&#x3D;，改成下面的内容（即修改成自己的安装路径）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/lib/jdk1.8.0_301</span><br></pre></td></tr></table></figure><p>再重新启动一次：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop</span><br><span class="line">./bin/hdfs namenode -format</span><br><span class="line">./sbin/start-dfs.sh</span><br></pre></td></tr></table></figure><p>这次就成功了，终端中输出如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Starting namenodes on [localhost]</span><br><span class="line">Starting datanodes</span><br><span class="line">Starting secondary namenodes [ubuntu]</span><br></pre></td></tr></table></figure><p>还可以在终端中输入jps命令，进一步检查各项进程是否都成功开启。如果没有，可能需要重新启动Hadoop再检查。成功示例如下（即包括NameNode、DataNode与SecondaryNameNode）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">3584 SecondaryNameNode</span><br><span class="line">3380 DataNode</span><br><span class="line">3209 NameNode</span><br><span class="line">3773 Jps</span><br></pre></td></tr></table></figure><p>若要关闭Hadoop进程，在终端中输入如下命令即可：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/stop-dfs.sh</span><br></pre></td></tr></table></figure><p>至此Hadoop的伪分布式配置就完成了，不过还可以接着做几件事，比如将Hadoop相关路径添加到环境变量中。打开终端，输入：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br></pre></td></tr></table></figure><p>在文件中添加Hadoop路径：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#hadoop</span><br><span class="line">export PATH=$PATH:/usr/local/hadoop/sbin:/usr/local/hadoop/bin</span><br></pre></td></tr></table></figure><p>更新配置文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure><p>这样就不需要每次都切换路径以启动Hadoop了，可以直接在终端中输入相关命令。例如，现在启动Hadoop，只需在终端中输入：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br></pre></td></tr></table></figure><p>输入下列命令可以在HDFS中创建hadoop用户：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -p /user/hadoop</span><br></pre></td></tr></table></figure><p>要查看HDFS中的文件，可输入以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -ls</span><br></pre></td></tr></table></figure><p>当然，现在什么文件都没有上传，这个命令也不会有任何返回。</p><p>另外，参考教程，在启动Hadoop服务后，我们可以在浏览器中输入localhost:50070查看信息。</p><p>当然，如果你在输入了之后浏览器告诉你无法访问，也是正常的，因为我也打不开🤡。怎么回事呢？</p><p>弄了半天也没头绪，最后查了一下，原来是Hadoop改端口号了🤭。教程里的Hadoop版本是2.7.1，而本次安装的版本为3.3.1。因此，实际上应该输入localhost:9870才是正确的端口号。</p><p>最后是关闭Hadoop服务。在终端中输入：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stop-dfs.sh</span><br></pre></td></tr></table></figure><p>正常情况下应该会返回类似以下输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Stopping namenodes on [localhost]</span><br><span class="line">Stopping datanodes</span><br><span class="line">Stopping secondary namenodes [ubuntu]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习记录（四）：Hadoop安装之一</title>
      <link href="/2022/03/07/spark4/"/>
      <url>/2022/03/07/spark4/</url>
      
        <content type="html"><![CDATA[<p>决定了安装的版本后（见上文），就是愉快的安装环节喽😋。</p><p>但在开始安装Hadoop前，参考教程，发现还有几个准备工作需要完成。</p><p>第一步，在Linux系统中添加hadoop用户。</p><p>以下流程均以Ubuntu系统为例。其他发行版类似操作，略微调整即可。打开终端输入：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo useradd -m hadoop -s /bin/bash</span><br></pre></td></tr></table></figure><p>即创建了一个hadoop用户，并以bash作为自己的shell。然后为hadoop用户设置密码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo passwd hadoop</span><br></pre></td></tr></table></figure><p>输入两遍想设置的密码即可，密码不会在终端中显示，不要因为看不到自己输入了什么就随便输（还真有这种事）。记住自己设置的密码。接下来赋予hadoop用户管理员权限，终端提示成功便可：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo adduser hadoop sudo</span><br></pre></td></tr></table></figure><p>第二步，配置ssh免密登陆。看教程需要这一步，那就配置一下。登录hadoop用户，首先更新一下系统，然后安装ssh server（Ubuntu默认安装了ssh client）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install openssh-server</span><br></pre></td></tr></table></figure><p>完成后再终端中输入下列命令登陆本机：（会有第一次登陆提示，输入yes后输入账户密码即可）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh localhost</span><br></pre></td></tr></table></figure><p>接下来配置ssh免密登录。在终端中输入下列代码即可（先exit退出ssh）。之后再使用ssh localhost登录时就不用输入密码了。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd ~/.ssh/</span><br><span class="line">ssh-keygen -t rsa</span><br><span class="line">cat ./id_rsa.pub &gt;&gt; ./authorized_keys</span><br></pre></td></tr></table></figure><p>经过上述操作后，查看教程发现下一步是安装Java。好在之前已经安装，然而当我在终端中输入java -version时，我惊奇的发现终端告诉我它找不到（结果如下）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Command &#x27;java&#x27; not found, but can be installed with:</span><br><span class="line"></span><br><span class="line">sudo apt install openjdk-11-jre-headless  # version 11.0.13+8-0ubuntu1~20.04, or</span><br><span class="line">sudo apt install default-jre              # version 2:1.11-72</span><br><span class="line">sudo apt install openjdk-13-jre-headless  # version 13.0.7+5-0ubuntu1~20.04</span><br><span class="line">sudo apt install openjdk-16-jre-headless  # version 16.0.1+9-1~20.04</span><br><span class="line">sudo apt install openjdk-17-jre-headless  # version 17.0.1+12-1~20.04</span><br><span class="line">sudo apt install openjdk-8-jre-headless   # version 8u312-b07-0ubuntu1~20.04</span><br></pre></td></tr></table></figure><p>可把我整乐了😅。</p><p>输入vim ~&#x2F;.bashrc，拉到最底下一看，原来是因为在hadoop用户的bash配置文件里没有Java的环境变量，加上即可（环境变量可参考之前Java安装一文）。</p><p>至此，安装Hadoop前的准备工作完成了🥰。</p><p>但仔细想想还没有。之前还安装过Scala，需要给hadoop用户操作权限（虽然不清楚这一步是否必需，还是弄一下比较好）。打开之前Scala安装的目录，输入下列代码即可：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local</span><br><span class="line">sudo chown -R hadoop ./scala</span><br></pre></td></tr></table></figure><p>然后在~&#x2F;.bashrc中添加Scala的环境变量（具体可见之前Scala安装一文）。最后在终端中输入scala检查。</p><p>现在，就来开始Hadoop的安装了。本文是在hadoop用户下进行操作的。到官网下载Hadoop的安装包（本文采用hadoop-3.3.1.tar.gz）。首先将安装包解压至想安装的位置，并重命名文件夹（在终端中输入），赋予权限：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo tar -zxf hadoop-3.3.1.tar.gz -C /usr/local</span><br><span class="line">cd /usr/local</span><br><span class="line">sudo mv ./hadoop-3.3.1 ./hadoop</span><br><span class="line">sudo chown -R hadoop ./hadoop</span><br></pre></td></tr></table></figure><p>然后检查Hadoop是否可用，在终端中输入：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop</span><br><span class="line">./bin/hadoop version</span><br></pre></td></tr></table></figure><p>得到下列输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Hadoop 3.3.1</span><br><span class="line">Source code repository https://github.com/apache/hadoop.git -r a3b9c37a397ad4188041dd80621bdeefc46885f2</span><br><span class="line">Compiled by ubuntu on 2021-06-15T05:13Z</span><br><span class="line">Compiled with protoc 3.7.1</span><br><span class="line">From source with checksum 88a4ddb2299aca054416d6b7f81ca55</span><br><span class="line">This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-3.3.1.jar</span><br></pre></td></tr></table></figure><p>到这一步还没有什么问题，之后将进行Hadoop的详细配置（单机配置与伪分布式配置）。</p>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习记录（三）：关于版本的问题</title>
      <link href="/2022/03/07/spark3/"/>
      <url>/2022/03/07/spark3/</url>
      
        <content type="html"><![CDATA[<p>安装完Java和Scala后，接下来就要进入Hadoop与Spark系统的搭建了。</p><p>但是重要的问题出现了，各环境之间的兼容性需要仔细考虑一下，不然可能会引起后续很多问题。本次学习参考的教程Hadoop的最新版本还是2.7.1，这个时候就很可怕了——之前计划安装的spark系统版本为3.x.y，并且基本打算安装最新的3.2.1。通过解压官网预构建Hadoop环境的Spark安装包发现，里面集成的Hadoop版本应该是3.3.1。</p><p><img src="/2022/03/07/spark3/2022-03-07-173019.png"></p><p>这是Hadoop 3.3.1安装包内截图，为什么放这张图？因为我想吐槽下下面那一张。</p><p>查阅学习的参考资料发现，教程所用环境基本较老，包括Spark版本也是2.x.y版本的。（众所周知，2.x.y与3.x.y基本是两种东西。）于是除了Hadoop版本之外，其他的一些组件，包括HBase，Hive等等，都得自己重新找互相兼容的版本了。</p><p>可这不是还没安装吗，怎么看是否兼容呢？</p><p>我不到啊。那么还能怎么办呢，继续看Spark安装包吧：</p><p><img src="/2022/03/07/spark3/2022-03-07-165234.png"></p><p>Spark-3.2.1-bin-with-hadoop-3.2安装包内截图（对照上图看看，怎么好意思说自己是3.2的，里面明明是3.3.1）</p><p>于是让人很难绷得住的事情发生了：这里集成的Hive版本怎么是2.3.9啊🤡。为什么这么说？因为Hive是3.x.y对应Hadoop的3.x.y的。不信吗？看看人家的发行说明：</p><p><img src="/2022/03/07/spark3/2022-03-07-181300.png"></p><p><img src="/2022/03/07/spark3/2022-03-07-181053.png"></p><p>看到这里，我忽然开始怀疑自己了：Spark安装包那张图上说的2.3.9，就一定是2.3.9了吗？可能这个2.3.9并不是说的Hive版本呢？</p><p>于是我分别去Hive官网下载了2.3.9和3.1.2（理论上对应Hadoop 3.x.y版本的版本），打开安装包，发现现实总是残酷的：</p><p><img src="/2022/03/07/spark3/2022-03-07-171356.png" alt="Hive 2.3.9"></p><p><img src="/2022/03/07/spark3/2022-03-07-171510.png" alt="Hive 3.1.2"></p><p>行了，Spark安装包里集成的环境就是Hive 2.3.9了😥。</p><p>那HBase呢？</p><p>到HBase的官方文档处找了半天，还是没找到和Hadoop版本对应的部分，然后我想起了有个东西叫网页查找。于是找到了这样一段话：</p><p><img src="/2022/03/07/spark3/2022-03-07-182712.jpg"></p><p>谢谢你，HBase！还有网页查找功能！🥰🥰🥰</p><p>然后是HBase和Java的兼容性，忘了截图了，但Jdk8和11对于新一点的版本在兼容性上是基本没有问题的，太好了，不然从一开始的工作全部要……</p><p>HBase还提了一下Zookeeper的版本要求，很简单：越新越好，至少3.4.x。看了Spark安装包（集成了环境的那个）里的是3.6.2。</p><p>那么本次学习的各软件版本基本可以确定为以下几个了：（如果这次没分析对，之后出了问题再说吧）</p><table><thead><tr><th align="center">软件</th><th align="center">版本</th></tr></thead><tbody><tr><td align="center">Spark</td><td align="center">3.2.1</td></tr><tr><td align="center">Hadoop</td><td align="center">3.3.1（虽然槽点很多）</td></tr><tr><td align="center">Hive</td><td align="center">2.3.9（槽点依旧很多）</td></tr><tr><td align="center">HBase</td><td align="center">2.4.10（先用着看一看）</td></tr><tr><td align="center">Scala</td><td align="center">2.12.15（上文提过了）</td></tr><tr><td align="center">Zookeeper</td><td align="center">3.6.2（暂定，反正HBase说越新越好）</td></tr></tbody></table><p>理论分析，希望没事🙏。</p>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习记录（二）：Scala安装</title>
      <link href="/2022/03/04/spark2/"/>
      <url>/2022/03/04/spark2/</url>
      
        <content type="html"><![CDATA[<p>Spark系统是基于Scala语言构建的，因此除了Java之外，也需要安装一下Scala。</p><p>安装Scala其实也很简单，值得注意的是安装的Scala版本并非版本越新越好，而要考虑是否与安装的Java版本和计划安装的Spark版本相兼容，不然可能会在之后出现各种问题，让自己觉得自己是个🤡。</p><p>目前最新的spark版本为3.2.x，一般来说对应安装2.12.x版本的Scala。</p><blockquote><p>摘自<a href="https://spark.apache.org/docs/latest/index.html">spark官方文档</a>的一段话：</p><p>Spark runs on Java 8&#x2F;11, Scala 2.12&#x2F;2.13, Python 3.6+ and R 3.5+. Python 3.6 support is deprecated as of Spark 3.2.0. Java 8 prior to version 8u201 support is deprecated as of Spark 3.2.0. For the Scala API, Spark 3.2.1 uses Scala 2.12. You will need to use a compatible Scala version (2.12.x).</p></blockquote><p>JDK各版本与Scala各版本对应关系见下表。</p><table><thead><tr><th align="center">JDK Version</th><th align="center">Minimum Scala Versions</th><th align="center">Recommended Scala Versions</th></tr></thead><tbody><tr><td align="center">18</td><td align="center">2.13.7, 2.12.15</td><td align="center">2.13.8, 2.12.15</td></tr><tr><td align="center">17</td><td align="center">2.13.6, 2.12.15</td><td align="center">2.13.8, 2.12.15</td></tr><tr><td align="center">11</td><td align="center">2.13.0, 2.12.4, 2.11.12</td><td align="center">2.13.8, 2.12.15, 2.11.12</td></tr><tr><td align="center">8</td><td align="center">2.13.0, 2.12.0, 2.11.0, 2.10.2</td><td align="center">2.13.8, 2.12.15, 2.11.12, 2.10.7</td></tr><tr><td align="center">6,7</td><td align="center">2.11.0, 2.10.0</td><td align="center">2.11.12, 2.10.7</td></tr></tbody></table><p>本文综合了自己安装的jdk版本（jdk8和jdk11），又考虑到上表以及官方文档内的说明，同时，又根据Spark官网下载的集成了Hadoop与Scala的Spark压缩包内jar文件夹里的Scala版本，最终决定安装2.12.15版本的Scala。</p><p>没错，Spark官网提供集成了依赖环境的安装包，等于是自己不用再安装Scala和Hadoop就能直接用Spark了，至福🤤。然而出于学习整体架构的需要，还是得自己手动安装和配置Scala和Hadoop😢。</p><p>接下来就是愉快的安装流程了捏😋。</p><p>至Scala官网下载安装包，一般是tgz压缩包格式，将此压缩包解压至想安装的的路径即可。</p><p>解压及重命名代码可见下例（在终端中输入，注意解压命令里的压缩包路径按实际情况来，至于解压的目的路径，本文选择&#x2F;usr&#x2F;local文件夹。别忘了这些安装路径，之后可能还会用到）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo tar -zxf ~/Desktop/scala-2.12.15.tgz -C /usr/local</span><br><span class="line">cd /usr/local/</span><br><span class="line">sudo mv ./scala-2.12.15/ ./scala</span><br></pre></td></tr></table></figure><p>别忘了添加环境变量😆。以Ubuntu系统为例，修改<del>&#x2F;.bashrc文件；如果是Manjaro系统，由于默认的终端为zsh而非bash，需要修改</del>&#x2F;.zshrc文件。总之，添加以下这一行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PATH=$PATH:/usr/local/scala/bin</span><br></pre></td></tr></table></figure><p>修改之后，更新一下配置文件。在终端中输入：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># choose your own system :)</span><br><span class="line">source ~/.bashrc</span><br><span class="line">source ~/.zshrc</span><br></pre></td></tr></table></figure><p>完成后在终端输入scala命令可以检查安装是否完成。出现以下输出，说明安装成功。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Welcome to Scala 2.12.15 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_301).</span><br><span class="line">Type in expressions for evaluation. Or try :help.</span><br><span class="line"></span><br><span class="line">scala&gt; </span><br></pre></td></tr></table></figure><p>附：小记一下利用IDEA编写Scala代码的方法。</p><p>在Linux系统中安装 IntelliJ IDEA（可以使用系统自带的应用商店或包管理器安装，也可以去官网下载安装包后自行安装，这里用的是Community Edition），在插件市场中搜索并安装Scala插件，重启ide。</p><p>然后就可以新建Scala项目了，如果是像上文写的那样自行安装Scala的话，在使用库中选中自己的安装路径，例如本文就是&#x2F;usr&#x2F;local&#x2F;scala，ide会自动识别sdk版本。之后就同新建Java项目一样了，于此不再赘述。</p>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习记录（一）：Java安装</title>
      <link href="/2022/03/04/spark1/"/>
      <url>/2022/03/04/spark1/</url>
      
        <content type="html"><![CDATA[<p>Spark系统需要运行在Java环境中，因此本文先写Linux系统下Java的安装。</p><p>Java的安装其实很简单。Open jdk或者Oracle jdk都可以，只需要注意一下自己安装的jdk版本和打算安装的Hadoop版本，Scala版本和Spark版本是否兼容。一般来说安装的不是上古版本都行，jdk8和jdk11是比较常见的两个版本。</p><p>如果是openjdk，可以直接通过命令行安装。</p><p>以Ubuntu系统安装openjdk8为例，打开终端输入命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install openjdk-8-jdk</span><br></pre></td></tr></table></figure><p>若使用的Linux系统为Arch系发行版，则包下载命令会与Ubuntu等Debian系发行版有所不同。</p><p>以Arch系安装openjdk8为例，可在终端输入以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo pacman -S jdk8-openjdk</span><br></pre></td></tr></table></figure><p>Arch系发行版环境下，如果觉得之前装的版本还不够，想再装一个版本的，可以使用archlinux-java命令切换版本（虽然一般不会有这个需求，但还是记一下🤗）。</p><p>以切换到openjdk11为例，在终端中输入：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo archlinux-java set java-11-openjdk</span><br></pre></td></tr></table></figure><p>本文采用Oracle版本进行安装。</p><p>首先需要先去官网下载压缩包（一般是tar.gz格式），再解压到想安装的路径。</p><p>解压安装包之后（有时通过命令行安装也需要），还需要编辑环境变量。常用的Ubuntu系统一般需要在~&#x2F;.bashrc文件中编辑好环境变量，也有教程在&#x2F;etc&#x2F;profile处修改，总之修改后都可以检查一下安装是否成功（检查方法下文会提到）。</p><p>Java环境变量的内容可参考下例（注意将JAVA_HOME中的路径改为自己的安装路径）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/lib/jdk1.8.0_301</span><br><span class="line">export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib</span><br><span class="line">export PATH=.:$&#123;JAVA_HOME&#125;/bin:$PATH</span><br></pre></td></tr></table></figure><p>注：忘了自己把Java装哪的话，可以在终端输入which java命令进行查找。（不过这个命令不一定适用于下载压缩包后自行安装的情形，有时会没有任何返回结果，即找不到。或许该命令更适用于在通过命令行进行安装时？）</p><p>完成上述流程后，可通过终端输入java -version命令来检查自身版本，也能起到检查安装是否正确完成的作用。成功安装后的输出结果一般如下所示（以Oracle版的jdk8为例）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">java version &quot;1.8.0_301&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_301-b09)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.301-b09, mixed mode)</span><br></pre></td></tr></table></figure><p>至此Java的安装就算完成了，之后将进行Scala，Hadoop和Spark等的安装与配置。</p>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World！</title>
      <link href="/2021/12/22/hello-world/"/>
      <url>/2021/12/22/hello-world/</url>
      
        <content type="html"><![CDATA[<p><img src="/2021/12/22/hello-world/%E5%B1%B1%E6%B0%B4.png"></p><p>Hello World</p><p>-—–2021.12.22</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
